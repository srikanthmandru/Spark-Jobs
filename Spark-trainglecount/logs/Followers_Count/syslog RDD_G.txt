(base) Srikanths-MacBook-Pro:Spark-trainglecount srikanthmandru$ make local
mvn clean package
[INFO] Scanning for projects...
[INFO] 
[INFO] -------------------------< cs6240:spark-demo >--------------------------
[INFO] Building spark-demo 1.0
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-demo ---
[INFO] Deleting /Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/target
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ spark-demo ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ spark-demo ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- scala-maven-plugin:3.3.1:compile (default) @ spark-demo ---
[WARNING]  Expected all dependencies to require Scala version: 2.11.12
[WARNING]  cs6240:spark-demo:1.0 requires scala version: 2.11.12
[WARNING]  com.twitter:chill_2.11:0.9.3 requires scala version: 2.11.12
[WARNING]  org.apache.spark:spark-core_2.11:2.4.5 requires scala version: 2.11.12
[WARNING]  org.json4s:json4s-jackson_2.11:3.5.3 requires scala version: 2.11.11
[WARNING] Multiple versions of scala libraries detected!
[INFO] /Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/src/main/scala:-1: info: compiling
[INFO] Compiling 10 source files to /Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/target/classes at 1582926697907
[INFO] prepare-compile in 0 s
[INFO] compile in 4 s
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ spark-demo ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ spark-demo ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ spark-demo ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ spark-demo ---
[INFO] Building jar: /Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/target/spark-demo-1.0.jar
[INFO] 
[INFO] --- maven-shade-plugin:3.1.0:shade (default) @ spark-demo ---
[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing /Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/target/spark-demo-1.0.jar with /Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/target/spark-demo-1.0-shaded.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  7.011 s
[INFO] Finished at: 2020-02-28T16:51:42-05:00
[INFO] ------------------------------------------------------------------------
cp target/spark-demo-1.0.jar spark-demo.jar
rm -rf output*
spark-submit --class wc.FollowerCount_RDDG --master local[6] --name "Triangle Count" spark-demo.jar input output R
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/Users/srikanthmandru/my-system/spark/spark-2.4.5-hadoop/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/Users/srikanthmandru/my-system/hadoop/hadoop-2.9.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
20/02/28 16:51:44 WARN util.Utils: Your hostname, Srikanths-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.110.81.16 instead (on interface en0)
20/02/28 16:51:44 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
20/02/28 16:51:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/02/28 16:51:44 INFO spark.SparkContext: Running Spark version 2.4.5
20/02/28 16:51:44 INFO spark.SparkContext: Submitted application: Word Count
20/02/28 16:51:44 INFO spark.SecurityManager: Changing view acls to: srikanthmandru
20/02/28 16:51:44 INFO spark.SecurityManager: Changing modify acls to: srikanthmandru
20/02/28 16:51:44 INFO spark.SecurityManager: Changing view acls groups to: 
20/02/28 16:51:44 INFO spark.SecurityManager: Changing modify acls groups to: 
20/02/28 16:51:44 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(srikanthmandru); groups with view permissions: Set(); users  with modify permissions: Set(srikanthmandru); groups with modify permissions: Set()
20/02/28 16:51:44 INFO util.Utils: Successfully started service 'sparkDriver' on port 63058.
20/02/28 16:51:44 INFO spark.SparkEnv: Registering MapOutputTracker
20/02/28 16:51:44 INFO spark.SparkEnv: Registering BlockManagerMaster
20/02/28 16:51:44 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/02/28 16:51:44 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/02/28 16:51:44 INFO storage.DiskBlockManager: Created local directory at /private/var/folders/l1/_8wm1c852jqf0btwzdcc0m4r0000gn/T/blockmgr-ff253bd6-577a-4490-9507-9c58873bbe60
20/02/28 16:51:44 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
20/02/28 16:51:44 INFO spark.SparkEnv: Registering OutputCommitCoordinator
20/02/28 16:51:44 INFO util.log: Logging initialized @1767ms
20/02/28 16:51:45 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
20/02/28 16:51:45 INFO server.Server: Started @1849ms
20/02/28 16:51:45 INFO server.AbstractConnector: Started ServerConnector@240139e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20/02/28 16:51:45 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21ae6e73{/jobs,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@13e9f2e2{/jobs/json,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@673bb956{/jobs/job,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60e949e1{/jobs/job/json,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c4bc9fc{/stages,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@680362a{/stages/json,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3569edd5{/stages/stage,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a356a0d{/stages/stage/json,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c827db{/stages/pool,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@377c68c6{/stages/pool/json,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@538cd0f2{/storage,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@238ad8c{/storage/json,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@430fa4ef{/storage/rdd,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1761de10{/storage/rdd/json,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22df874e{/environment,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@654c1a54{/environment/json,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bdaf2ce{/executors,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@42d236fb{/executors/json,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ce93c18{/executors/threadDump,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19f21b6b{/executors/threadDump/json,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1532c619{/static,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65e7f52a{/,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@304b9f1a{/api,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@263558c9{/jobs/job/kill,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f14f20c{/stages/stage/kill,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.110.81.16:4040
20/02/28 16:51:45 INFO spark.SparkContext: Added JAR file:/Users/srikanthmandru/my-system/semester%203/lsp/homeworks/HW3/Spark-trainglecount/spark-demo.jar at spark://10.110.81.16:63058/jars/spark-demo.jar with timestamp 1582926705122
20/02/28 16:51:45 INFO executor.Executor: Starting executor ID driver on host localhost
20/02/28 16:51:45 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63059.
20/02/28 16:51:45 INFO netty.NettyBlockTransferService: Server created on 10.110.81.16:63059
20/02/28 16:51:45 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/02/28 16:51:45 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.110.81.16, 63059, None)
20/02/28 16:51:45 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.110.81.16:63059 with 366.3 MB RAM, BlockManagerId(driver, 10.110.81.16, 63059, None)
20/02/28 16:51:45 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.110.81.16, 63059, None)
20/02/28 16:51:45 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.110.81.16, 63059, None)
20/02/28 16:51:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@676ff3b0{/metrics/json,null,AVAILABLE,@Spark}
20/02/28 16:51:45 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 236.7 KB, free 366.1 MB)
20/02/28 16:51:46 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.0 MB)
20/02/28 16:51:46 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.110.81.16:63059 (size: 22.9 KB, free: 366.3 MB)
20/02/28 16:51:46 INFO spark.SparkContext: Created broadcast 0 from textFile at FollowerCount_RDDG.scala:25
20/02/28 16:51:46 INFO mapred.FileInputFormat: Total input paths to process : 1
(Debug info :,(10) CoalescedRDD[7] at coalesce at FollowerCount_RDDG.scala:29 []
 |   MapPartitionsRDD[6] at map at FollowerCount_RDDG.scala:29 []
 |   ShuffledRDD[5] at groupByKey at FollowerCount_RDDG.scala:28 []
 +-(40) MapPartitionsRDD[4] at map at FollowerCount_RDDG.scala:27 []
    |   MapPartitionsRDD[3] at flatMap at FollowerCount_RDDG.scala:26 []
    |   MapPartitionsRDD[2] at flatMap at FollowerCount_RDDG.scala:26 []
    |   input MapPartitionsRDD[1] at textFile at FollowerCount_RDDG.scala:25 []
    |   input HadoopRDD[0] at textFile at FollowerCount_RDDG.scala:25 [])
20/02/28 16:51:46 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
20/02/28 16:51:46 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
20/02/28 16:51:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/02/28 16:51:46 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
20/02/28 16:51:46 INFO spark.SparkContext: RDD's recursive dependencies:
(10) MapPartitionsRDD[8] at saveAsTextFile at FollowerCount_RDDG.scala:32 []
 |   CoalescedRDD[7] at coalesce at FollowerCount_RDDG.scala:29 []
 |   MapPartitionsRDD[6] at map at FollowerCount_RDDG.scala:29 []
 |   ShuffledRDD[5] at groupByKey at FollowerCount_RDDG.scala:28 []
 +-(40) MapPartitionsRDD[4] at map at FollowerCount_RDDG.scala:27 []
    |   MapPartitionsRDD[3] at flatMap at FollowerCount_RDDG.scala:26 []
    |   MapPartitionsRDD[2] at flatMap at FollowerCount_RDDG.scala:26 []
    |   input MapPartitionsRDD[1] at textFile at FollowerCount_RDDG.scala:25 []
    |   input HadoopRDD[0] at textFile at FollowerCount_RDDG.scala:25 []
20/02/28 16:51:46 INFO scheduler.DAGScheduler: Registering RDD 4 (map at FollowerCount_RDDG.scala:27) as input to shuffle 0
20/02/28 16:51:46 INFO scheduler.DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:78) with 10 output partitions
20/02/28 16:51:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:78)
20/02/28 16:51:46 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/02/28 16:51:46 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/02/28 16:51:46 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at map at FollowerCount_RDDG.scala:27), which has no missing parents
20/02/28 16:51:46 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.1 KB, free 366.0 MB)
20/02/28 16:51:46 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 366.0 MB)
20/02/28 16:51:46 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.110.81.16:63059 (size: 3.3 KB, free: 366.3 MB)
20/02/28 16:51:46 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163
20/02/28 16:51:46 INFO scheduler.DAGScheduler: Submitting 40 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at map at FollowerCount_RDDG.scala:27) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/02/28 16:51:46 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 40 tasks
20/02/28 16:51:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:46 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:46 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:46 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:46 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:46 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:46 INFO executor.Executor: Running task 5.0 in stage 0.0 (TID 5)
20/02/28 16:51:46 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
20/02/28 16:51:46 INFO executor.Executor: Running task 4.0 in stage 0.0 (TID 4)
20/02/28 16:51:46 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
20/02/28 16:51:46 INFO executor.Executor: Running task 3.0 in stage 0.0 (TID 3)
20/02/28 16:51:46 INFO executor.Executor: Running task 2.0 in stage 0.0 (TID 2)
20/02/28 16:51:46 INFO executor.Executor: Fetching spark://10.110.81.16:63058/jars/spark-demo.jar with timestamp 1582926705122
20/02/28 16:51:46 INFO client.TransportClientFactory: Successfully created connection to /10.110.81.16:63058 after 31 ms (0 ms spent in bootstraps)
20/02/28 16:51:46 INFO util.Utils: Fetching spark://10.110.81.16:63058/jars/spark-demo.jar to /private/var/folders/l1/_8wm1c852jqf0btwzdcc0m4r0000gn/T/spark-d4603fe6-8b5c-45b3-a544-dabe7e7f8485/userFiles-2aea8596-bd53-4708-8585-562e96704148/fetchFileTemp2896068212306806261.tmp
20/02/28 16:51:46 INFO executor.Executor: Adding file:/private/var/folders/l1/_8wm1c852jqf0btwzdcc0m4r0000gn/T/spark-d4603fe6-8b5c-45b3-a544-dabe7e7f8485/userFiles-2aea8596-bd53-4708-8585-562e96704148/spark-demo.jar to class loader
20/02/28 16:51:46 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:33554432+33554432
20/02/28 16:51:46 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:0+33554432
20/02/28 16:51:46 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:100663296+33554432
20/02/28 16:51:46 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:167772160+33554432
20/02/28 16:51:46 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:67108864+33554432
20/02/28 16:51:46 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:134217728+33554432
20/02/28 16:51:49 INFO executor.Executor: Finished task 5.0 in stage 0.0 (TID 5). 1106 bytes result sent to driver
20/02/28 16:51:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:49 INFO executor.Executor: Running task 6.0 in stage 0.0 (TID 6)
20/02/28 16:51:49 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:201326592+33554432
20/02/28 16:51:49 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 3354 ms on localhost (executor driver) (1/40)
20/02/28 16:51:49 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 1063 bytes result sent to driver
20/02/28 16:51:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:49 INFO executor.Executor: Running task 7.0 in stage 0.0 (TID 7)
20/02/28 16:51:49 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3361 ms on localhost (executor driver) (2/40)
20/02/28 16:51:49 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:234881024+33554432
20/02/28 16:51:49 INFO executor.Executor: Finished task 3.0 in stage 0.0 (TID 3). 1063 bytes result sent to driver
20/02/28 16:51:49 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:49 INFO executor.Executor: Running task 8.0 in stage 0.0 (TID 8)
20/02/28 16:51:49 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 3374 ms on localhost (executor driver) (3/40)
20/02/28 16:51:49 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:268435456+33554432
20/02/28 16:51:49 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1063 bytes result sent to driver
20/02/28 16:51:49 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:49 INFO executor.Executor: Running task 9.0 in stage 0.0 (TID 9)
20/02/28 16:51:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3479 ms on localhost (executor driver) (4/40)
20/02/28 16:51:49 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:301989888+33554432
20/02/28 16:51:49 INFO executor.Executor: Finished task 4.0 in stage 0.0 (TID 4). 1063 bytes result sent to driver
20/02/28 16:51:49 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:49 INFO executor.Executor: Running task 10.0 in stage 0.0 (TID 10)
20/02/28 16:51:49 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 3518 ms on localhost (executor driver) (5/40)
20/02/28 16:51:49 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:335544320+33554432
20/02/28 16:51:49 INFO executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 1063 bytes result sent to driver
20/02/28 16:51:49 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:49 INFO executor.Executor: Running task 11.0 in stage 0.0 (TID 11)
20/02/28 16:51:49 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 3529 ms on localhost (executor driver) (6/40)
20/02/28 16:51:49 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:369098752+33554432
20/02/28 16:51:52 INFO executor.Executor: Finished task 7.0 in stage 0.0 (TID 7). 1063 bytes result sent to driver
20/02/28 16:51:52 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:52 INFO executor.Executor: Running task 12.0 in stage 0.0 (TID 12)
20/02/28 16:51:52 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 2981 ms on localhost (executor driver) (7/40)
20/02/28 16:51:52 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:402653184+33554432
20/02/28 16:51:52 INFO executor.Executor: Finished task 8.0 in stage 0.0 (TID 8). 1063 bytes result sent to driver
20/02/28 16:51:52 INFO executor.Executor: Finished task 6.0 in stage 0.0 (TID 6). 1063 bytes result sent to driver
20/02/28 16:51:52 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:52 INFO executor.Executor: Running task 13.0 in stage 0.0 (TID 13)
20/02/28 16:51:52 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:52 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 3022 ms on localhost (executor driver) (8/40)
20/02/28 16:51:52 INFO executor.Executor: Running task 14.0 in stage 0.0 (TID 14)
20/02/28 16:51:52 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 3045 ms on localhost (executor driver) (9/40)
20/02/28 16:51:52 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:436207616+33554432
20/02/28 16:51:52 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:469762048+33554432
20/02/28 16:51:52 INFO executor.Executor: Finished task 10.0 in stage 0.0 (TID 10). 1020 bytes result sent to driver
20/02/28 16:51:52 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15, localhost, executor driver, partition 15, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:52 INFO executor.Executor: Running task 15.0 in stage 0.0 (TID 15)
20/02/28 16:51:52 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 2979 ms on localhost (executor driver) (10/40)
20/02/28 16:51:52 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:503316480+33554432
20/02/28 16:51:53 INFO executor.Executor: Finished task 9.0 in stage 0.0 (TID 9). 1020 bytes result sent to driver
20/02/28 16:51:53 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16, localhost, executor driver, partition 16, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:53 INFO executor.Executor: Running task 16.0 in stage 0.0 (TID 16)
20/02/28 16:51:53 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 3127 ms on localhost (executor driver) (11/40)
20/02/28 16:51:53 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:536870912+33554432
20/02/28 16:51:53 INFO executor.Executor: Finished task 11.0 in stage 0.0 (TID 11). 1063 bytes result sent to driver
20/02/28 16:51:53 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17, localhost, executor driver, partition 17, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:53 INFO executor.Executor: Running task 17.0 in stage 0.0 (TID 17)
20/02/28 16:51:53 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 3082 ms on localhost (executor driver) (12/40)
20/02/28 16:51:53 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:570425344+33554432
20/02/28 16:51:55 INFO executor.Executor: Finished task 15.0 in stage 0.0 (TID 15). 1063 bytes result sent to driver
20/02/28 16:51:55 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18, localhost, executor driver, partition 18, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:55 INFO executor.Executor: Running task 18.0 in stage 0.0 (TID 18)
20/02/28 16:51:55 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 3013 ms on localhost (executor driver) (13/40)
20/02/28 16:51:55 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:603979776+33554432
20/02/28 16:51:56 INFO executor.Executor: Finished task 12.0 in stage 0.0 (TID 12). 1063 bytes result sent to driver
20/02/28 16:51:56 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19, localhost, executor driver, partition 19, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:56 INFO executor.Executor: Running task 19.0 in stage 0.0 (TID 19)
20/02/28 16:51:56 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 3214 ms on localhost (executor driver) (14/40)
20/02/28 16:51:56 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:637534208+33554432
20/02/28 16:51:56 INFO executor.Executor: Finished task 14.0 in stage 0.0 (TID 14). 1020 bytes result sent to driver
20/02/28 16:51:56 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20, localhost, executor driver, partition 20, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:56 INFO executor.Executor: Running task 20.0 in stage 0.0 (TID 20)
20/02/28 16:51:56 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 3228 ms on localhost (executor driver) (15/40)
20/02/28 16:51:56 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:671088640+33554432
20/02/28 16:51:56 INFO executor.Executor: Finished task 17.0 in stage 0.0 (TID 17). 1063 bytes result sent to driver
20/02/28 16:51:56 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21, localhost, executor driver, partition 21, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:56 INFO executor.Executor: Running task 21.0 in stage 0.0 (TID 21)
20/02/28 16:51:56 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 3021 ms on localhost (executor driver) (16/40)
20/02/28 16:51:56 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:704643072+33554432
20/02/28 16:51:56 INFO executor.Executor: Finished task 16.0 in stage 0.0 (TID 16). 1063 bytes result sent to driver
20/02/28 16:51:56 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22, localhost, executor driver, partition 22, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:56 INFO executor.Executor: Running task 22.0 in stage 0.0 (TID 22)
20/02/28 16:51:56 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 3096 ms on localhost (executor driver) (17/40)
20/02/28 16:51:56 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:738197504+33554432
20/02/28 16:51:56 INFO executor.Executor: Finished task 13.0 in stage 0.0 (TID 13). 1063 bytes result sent to driver
20/02/28 16:51:56 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23, localhost, executor driver, partition 23, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:56 INFO executor.Executor: Running task 23.0 in stage 0.0 (TID 23)
20/02/28 16:51:56 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 3309 ms on localhost (executor driver) (18/40)
20/02/28 16:51:56 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:771751936+33554432
20/02/28 16:51:59 INFO executor.Executor: Finished task 18.0 in stage 0.0 (TID 18). 1063 bytes result sent to driver
20/02/28 16:51:59 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24, localhost, executor driver, partition 24, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:59 INFO executor.Executor: Running task 24.0 in stage 0.0 (TID 24)
20/02/28 16:51:59 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 3220 ms on localhost (executor driver) (19/40)
20/02/28 16:51:59 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:805306368+33554432
20/02/28 16:51:59 INFO executor.Executor: Finished task 19.0 in stage 0.0 (TID 19). 1063 bytes result sent to driver
20/02/28 16:51:59 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25, localhost, executor driver, partition 25, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:59 INFO executor.Executor: Running task 25.0 in stage 0.0 (TID 25)
20/02/28 16:51:59 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 3268 ms on localhost (executor driver) (20/40)
20/02/28 16:51:59 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:838860800+33554432
20/02/28 16:51:59 INFO executor.Executor: Finished task 21.0 in stage 0.0 (TID 21). 1063 bytes result sent to driver
20/02/28 16:51:59 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26, localhost, executor driver, partition 26, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:59 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 3205 ms on localhost (executor driver) (21/40)
20/02/28 16:51:59 INFO executor.Executor: Running task 26.0 in stage 0.0 (TID 26)
20/02/28 16:51:59 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:872415232+33554432
20/02/28 16:51:59 INFO executor.Executor: Finished task 23.0 in stage 0.0 (TID 23). 1063 bytes result sent to driver
20/02/28 16:51:59 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27, localhost, executor driver, partition 27, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:59 INFO executor.Executor: Running task 27.0 in stage 0.0 (TID 27)
20/02/28 16:51:59 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 3213 ms on localhost (executor driver) (22/40)
20/02/28 16:51:59 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:905969664+33554432
20/02/28 16:51:59 INFO executor.Executor: Finished task 20.0 in stage 0.0 (TID 20). 1063 bytes result sent to driver
20/02/28 16:51:59 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28, localhost, executor driver, partition 28, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:59 INFO executor.Executor: Running task 28.0 in stage 0.0 (TID 28)
20/02/28 16:51:59 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 3338 ms on localhost (executor driver) (23/40)
20/02/28 16:51:59 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:939524096+33554432
20/02/28 16:51:59 INFO executor.Executor: Finished task 22.0 in stage 0.0 (TID 22). 1063 bytes result sent to driver
20/02/28 16:51:59 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29, localhost, executor driver, partition 29, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:51:59 INFO executor.Executor: Running task 29.0 in stage 0.0 (TID 29)
20/02/28 16:51:59 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 3332 ms on localhost (executor driver) (24/40)
20/02/28 16:51:59 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:973078528+33554432
20/02/28 16:52:02 INFO executor.Executor: Finished task 25.0 in stage 0.0 (TID 25). 1020 bytes result sent to driver
20/02/28 16:52:02 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30, localhost, executor driver, partition 30, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:52:02 INFO executor.Executor: Running task 30.0 in stage 0.0 (TID 30)
20/02/28 16:52:02 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 2904 ms on localhost (executor driver) (25/40)
20/02/28 16:52:02 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:1006632960+33554432
20/02/28 16:52:02 INFO executor.Executor: Finished task 28.0 in stage 0.0 (TID 28). 1063 bytes result sent to driver
20/02/28 16:52:02 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31, localhost, executor driver, partition 31, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:52:02 INFO executor.Executor: Running task 31.0 in stage 0.0 (TID 31)
20/02/28 16:52:02 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 2830 ms on localhost (executor driver) (26/40)
20/02/28 16:52:02 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:1040187392+33554432
20/02/28 16:52:02 INFO executor.Executor: Finished task 24.0 in stage 0.0 (TID 24). 1063 bytes result sent to driver
20/02/28 16:52:02 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32, localhost, executor driver, partition 32, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:52:02 INFO executor.Executor: Running task 32.0 in stage 0.0 (TID 32)
20/02/28 16:52:02 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 3085 ms on localhost (executor driver) (27/40)
20/02/28 16:52:02 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:1073741824+33554432
20/02/28 16:52:02 INFO executor.Executor: Finished task 27.0 in stage 0.0 (TID 27). 1063 bytes result sent to driver
20/02/28 16:52:02 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33, localhost, executor driver, partition 33, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:52:02 INFO executor.Executor: Running task 33.0 in stage 0.0 (TID 33)
20/02/28 16:52:02 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 2966 ms on localhost (executor driver) (28/40)
20/02/28 16:52:02 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:1107296256+33554432
20/02/28 16:52:02 INFO executor.Executor: Finished task 29.0 in stage 0.0 (TID 29). 1063 bytes result sent to driver
20/02/28 16:52:02 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34, localhost, executor driver, partition 34, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:52:02 INFO executor.Executor: Running task 34.0 in stage 0.0 (TID 34)
20/02/28 16:52:02 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 2899 ms on localhost (executor driver) (29/40)
20/02/28 16:52:02 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:1140850688+33554432
20/02/28 16:52:02 INFO executor.Executor: Finished task 26.0 in stage 0.0 (TID 26). 1020 bytes result sent to driver
20/02/28 16:52:02 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35, localhost, executor driver, partition 35, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:52:02 INFO executor.Executor: Running task 35.0 in stage 0.0 (TID 35)
20/02/28 16:52:02 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 3082 ms on localhost (executor driver) (30/40)
20/02/28 16:52:02 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:1174405120+33554432
20/02/28 16:52:04 INFO executor.Executor: Finished task 30.0 in stage 0.0 (TID 30). 1063 bytes result sent to driver
20/02/28 16:52:04 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36, localhost, executor driver, partition 36, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:52:04 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 2719 ms on localhost (executor driver) (31/40)
20/02/28 16:52:04 INFO executor.Executor: Running task 36.0 in stage 0.0 (TID 36)
20/02/28 16:52:04 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:1207959552+33554432
20/02/28 16:52:05 INFO executor.Executor: Finished task 31.0 in stage 0.0 (TID 31). 1063 bytes result sent to driver
20/02/28 16:52:05 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37, localhost, executor driver, partition 37, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:52:05 INFO executor.Executor: Running task 37.0 in stage 0.0 (TID 37)
20/02/28 16:52:05 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 2774 ms on localhost (executor driver) (32/40)
20/02/28 16:52:05 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:1241513984+33554432
20/02/28 16:52:05 INFO executor.Executor: Finished task 32.0 in stage 0.0 (TID 32). 1020 bytes result sent to driver
20/02/28 16:52:05 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38, localhost, executor driver, partition 38, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:52:05 INFO executor.Executor: Running task 38.0 in stage 0.0 (TID 38)
20/02/28 16:52:05 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 2812 ms on localhost (executor driver) (33/40)
20/02/28 16:52:05 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:1275068416+33554432
20/02/28 16:52:05 INFO executor.Executor: Finished task 33.0 in stage 0.0 (TID 33). 1020 bytes result sent to driver
20/02/28 16:52:05 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39, localhost, executor driver, partition 39, PROCESS_LOCAL, 7950 bytes)
20/02/28 16:52:05 INFO executor.Executor: Running task 39.0 in stage 0.0 (TID 39)
20/02/28 16:52:05 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 2764 ms on localhost (executor driver) (34/40)
20/02/28 16:52:05 INFO rdd.HadoopRDD: Input split: file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/input/edges.csv:1308622848+10658977
20/02/28 16:52:05 INFO executor.Executor: Finished task 34.0 in stage 0.0 (TID 34). 1063 bytes result sent to driver
20/02/28 16:52:05 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 2774 ms on localhost (executor driver) (35/40)
20/02/28 16:52:05 INFO executor.Executor: Finished task 35.0 in stage 0.0 (TID 35). 1063 bytes result sent to driver
20/02/28 16:52:05 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 2876 ms on localhost (executor driver) (36/40)
20/02/28 16:52:05 INFO executor.Executor: Finished task 39.0 in stage 0.0 (TID 39). 1020 bytes result sent to driver
20/02/28 16:52:05 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 862 ms on localhost (executor driver) (37/40)
20/02/28 16:52:07 INFO executor.Executor: Finished task 36.0 in stage 0.0 (TID 36). 1063 bytes result sent to driver
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 2533 ms on localhost (executor driver) (38/40)
20/02/28 16:52:07 INFO executor.Executor: Finished task 37.0 in stage 0.0 (TID 37). 1063 bytes result sent to driver
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 2486 ms on localhost (executor driver) (39/40)
20/02/28 16:52:07 INFO executor.Executor: Finished task 38.0 in stage 0.0 (TID 38). 1020 bytes result sent to driver
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 2547 ms on localhost (executor driver) (40/40)
20/02/28 16:52:07 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/02/28 16:52:07 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (map at FollowerCount_RDDG.scala:27) finished in 21.268 s
20/02/28 16:52:07 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/02/28 16:52:07 INFO scheduler.DAGScheduler: running: Set()
20/02/28 16:52:07 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)
20/02/28 16:52:07 INFO scheduler.DAGScheduler: failed: Set()
20/02/28 16:52:07 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at FollowerCount_RDDG.scala:32), which has no missing parents
20/02/28 16:52:07 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 76.6 KB, free 366.0 MB)
20/02/28 16:52:07 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.2 KB, free 365.9 MB)
20/02/28 16:52:07 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.110.81.16:63059 (size: 28.2 KB, free: 366.2 MB)
20/02/28 16:52:07 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1163
20/02/28 16:52:07 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at FollowerCount_RDDG.scala:32) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
20/02/28 16:52:07 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 10 tasks
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 40, localhost, executor driver, partition 0, PROCESS_LOCAL, 7992 bytes)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 41, localhost, executor driver, partition 1, PROCESS_LOCAL, 7992 bytes)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 1.0 (TID 42, localhost, executor driver, partition 5, PROCESS_LOCAL, 7992 bytes)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 1.0 (TID 43, localhost, executor driver, partition 6, PROCESS_LOCAL, 7992 bytes)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 1.0 (TID 44, localhost, executor driver, partition 7, PROCESS_LOCAL, 7992 bytes)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 1.0 (TID 45, localhost, executor driver, partition 8, PROCESS_LOCAL, 7992 bytes)
20/02/28 16:52:07 INFO executor.Executor: Running task 7.0 in stage 1.0 (TID 44)
20/02/28 16:52:07 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 40)
20/02/28 16:52:07 INFO executor.Executor: Running task 6.0 in stage 1.0 (TID 43)
20/02/28 16:52:07 INFO executor.Executor: Running task 8.0 in stage 1.0 (TID 45)
20/02/28 16:52:07 INFO executor.Executor: Running task 5.0 in stage 1.0 (TID 42)
20/02/28 16:52:07 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 41)
20/02/28 16:52:07 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
20/02/28 16:52:07 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
20/02/28 16:52:07 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
20/02/28 16:52:07 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
20/02/28 16:52:07 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
20/02/28 16:52:07 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
20/02/28 16:52:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/02/28 16:52:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/02/28 16:52:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/02/28 16:52:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/02/28 16:52:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/02/28 16:52:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200228165146_0008_m_000005_0' to file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/output/_temporary/0/task_20200228165146_0008_m_000005
20/02/28 16:52:07 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200228165146_0008_m_000006_0' to file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/output/_temporary/0/task_20200228165146_0008_m_000006
20/02/28 16:52:07 INFO mapred.SparkHadoopMapRedUtil: attempt_20200228165146_0008_m_000005_0: Committed
20/02/28 16:52:07 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200228165146_0008_m_000001_0' to file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/output/_temporary/0/task_20200228165146_0008_m_000001
20/02/28 16:52:07 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200228165146_0008_m_000000_0' to file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/output/_temporary/0/task_20200228165146_0008_m_000000
20/02/28 16:52:07 INFO mapred.SparkHadoopMapRedUtil: attempt_20200228165146_0008_m_000000_0: Committed
20/02/28 16:52:07 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200228165146_0008_m_000007_0' to file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/output/_temporary/0/task_20200228165146_0008_m_000007
20/02/28 16:52:07 INFO mapred.SparkHadoopMapRedUtil: attempt_20200228165146_0008_m_000006_0: Committed
20/02/28 16:52:07 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200228165146_0008_m_000008_0' to file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/output/_temporary/0/task_20200228165146_0008_m_000008
20/02/28 16:52:07 INFO mapred.SparkHadoopMapRedUtil: attempt_20200228165146_0008_m_000007_0: Committed
20/02/28 16:52:07 INFO mapred.SparkHadoopMapRedUtil: attempt_20200228165146_0008_m_000001_0: Committed
20/02/28 16:52:07 INFO mapred.SparkHadoopMapRedUtil: attempt_20200228165146_0008_m_000008_0: Committed
20/02/28 16:52:07 INFO executor.Executor: Finished task 7.0 in stage 1.0 (TID 44). 1416 bytes result sent to driver
20/02/28 16:52:07 INFO executor.Executor: Finished task 5.0 in stage 1.0 (TID 42). 1416 bytes result sent to driver
20/02/28 16:52:07 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 40). 1416 bytes result sent to driver
20/02/28 16:52:07 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 41). 1416 bytes result sent to driver
20/02/28 16:52:07 INFO executor.Executor: Finished task 8.0 in stage 1.0 (TID 45). 1416 bytes result sent to driver
20/02/28 16:52:07 INFO executor.Executor: Finished task 6.0 in stage 1.0 (TID 43). 1416 bytes result sent to driver
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 1.0 (TID 46, localhost, executor driver, partition 9, PROCESS_LOCAL, 7992 bytes)
20/02/28 16:52:07 INFO executor.Executor: Running task 9.0 in stage 1.0 (TID 46)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 47, localhost, executor driver, partition 2, ANY, 7992 bytes)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 1.0 (TID 44) in 113 ms on localhost (executor driver) (1/10)
20/02/28 16:52:07 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 47)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 48, localhost, executor driver, partition 3, ANY, 7992 bytes)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 1.0 (TID 42) in 114 ms on localhost (executor driver) (2/10)
20/02/28 16:52:07 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 48)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 40) in 131 ms on localhost (executor driver) (3/10)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 49, localhost, executor driver, partition 4, ANY, 7992 bytes)
20/02/28 16:52:07 INFO executor.Executor: Running task 4.0 in stage 1.0 (TID 49)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 41) in 115 ms on localhost (executor driver) (4/10)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 1.0 (TID 45) in 115 ms on localhost (executor driver) (5/10)
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 1.0 (TID 43) in 115 ms on localhost (executor driver) (6/10)
20/02/28 16:52:07 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
20/02/28 16:52:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/02/28 16:52:07 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
20/02/28 16:52:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/02/28 16:52:07 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
20/02/28 16:52:07 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
20/02/28 16:52:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/02/28 16:52:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 40 non-empty blocks including 40 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 40 non-empty blocks including 40 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 40 non-empty blocks including 40 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks
20/02/28 16:52:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/02/28 16:52:07 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200228165146_0008_m_000009_0' to file:/Users/srikanthmandru/my-system/semester 3/lsp/homeworks/HW3/Spark-trainglecount/output/_temporary/0/task_20200228165146_0008_m_000009
20/02/28 16:52:07 INFO mapred.SparkHadoopMapRedUtil: attempt_20200228165146_0008_m_000009_0: Committed
20/02/28 16:52:07 INFO executor.Executor: Finished task 9.0 in stage 1.0 (TID 46). 1373 bytes result sent to driver
20/02/28 16:52:07 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 1.0 (TID 46) in 54 ms on localhost (executor driver) (7/10)
20/02/28 16:52:08 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.110.81.16:63059 in memory (size: 3.3 KB, free: 366.3 MB)
20/02/28 16:52:10 INFO collection.ExternalAppendOnlyMap: Thread 63 spilling in-memory map of 128.0 MB to disk (1 time so far)
20/02/28 16:52:10 INFO collection.ExternalAppendOnlyMap: Thread 60 spilling in-memory map of 128.0 MB to disk (1 time so far)
20/02/28 16:52:10 INFO collection.ExternalAppendOnlyMap: Thread 64 spilling in-memory map of 128.0 MB to disk (1 time so far)
20/02/28 16:52:14 INFO collection.ExternalAppendOnlyMap: Thread 63 spilling in-memory map of 128.0 MB to disk (2 times so far)
20/02/28 16:52:14 INFO collection.ExternalAppendOnlyMap: Thread 60 spilling in-memory map of 128.0 MB to disk (2 times so far)
20/02/28 16:52:14 INFO collection.ExternalAppendOnlyMap: Thread 64 spilling in-memory map of 188.0 MB to disk (2 times so far)
20/02/28 16:52:16 ERROR util.Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:117)
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:115)
        at org.apache.spark.util.collection.CompactBuffer.growToSize(CompactBuffer.scala:146)
        at org.apache.spark.util.collection.CompactBuffer.$plus$plus$eq(CompactBuffer.scala:92)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.mergeIfKeyExists(ExternalAppendOnlyMap.scala:355)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:402)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:302)
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at scala.collection.Iterator$$anon$12.next(Iterator.scala:445)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:131)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:123)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
20/02/28 16:52:16 ERROR io.SparkHadoopWriter: Task attempt_20200228165146_0008_m_000002_0 aborted.
20/02/28 16:52:16 ERROR executor.Executor: Exception in task 2.0 in stage 1.0 (TID 47)
org.apache.spark.SparkException: Task failed while writing rows
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:123)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:117)
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:115)
        at org.apache.spark.util.collection.CompactBuffer.growToSize(CompactBuffer.scala:146)
        at org.apache.spark.util.collection.CompactBuffer.$plus$plus$eq(CompactBuffer.scala:92)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.mergeIfKeyExists(ExternalAppendOnlyMap.scala:355)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:402)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:302)
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at scala.collection.Iterator$$anon$12.next(Iterator.scala:445)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:131)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)
        ... 10 more
20/02/28 16:52:16 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 1.0 (TID 47, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:123)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:117)
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:115)
        at org.apache.spark.util.collection.CompactBuffer.growToSize(CompactBuffer.scala:146)
        at org.apache.spark.util.collection.CompactBuffer.$plus$plus$eq(CompactBuffer.scala:92)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.mergeIfKeyExists(ExternalAppendOnlyMap.scala:355)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:402)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:302)
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at scala.collection.Iterator$$anon$12.next(Iterator.scala:445)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:131)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)
        ... 10 more

20/02/28 16:52:16 ERROR scheduler.TaskSetManager: Task 2 in stage 1.0 failed 1 times; aborting job
20/02/28 16:52:16 INFO scheduler.TaskSchedulerImpl: Cancelling stage 1
20/02/28 16:52:16 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled
20/02/28 16:52:16 INFO executor.Executor: Executor is trying to kill task 3.0 in stage 1.0 (TID 48), reason: Stage cancelled
20/02/28 16:52:16 INFO executor.Executor: Executor is trying to kill task 4.0 in stage 1.0 (TID 49), reason: Stage cancelled
20/02/28 16:52:16 ERROR util.Utils: Aborting task
org.apache.spark.TaskKilledException
        at org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:149)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:156)
        at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)
        at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:100)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:99)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:130)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:123)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
20/02/28 16:52:16 INFO scheduler.TaskSchedulerImpl: Stage 1 was cancelled
20/02/28 16:52:16 ERROR util.Utils: Aborting task
org.apache.spark.TaskKilledException
        at org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:149)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:156)
        at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:41)
        at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:90)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:100)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:99)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:130)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:123)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
20/02/28 16:52:16 ERROR io.SparkHadoopWriter: Task attempt_20200228165146_0008_m_000003_0 aborted.
20/02/28 16:52:16 INFO scheduler.DAGScheduler: ResultStage 1 (runJob at SparkHadoopWriter.scala:78) failed in 8.788 s due to Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 47, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:123)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:117)
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:115)
        at org.apache.spark.util.collection.CompactBuffer.growToSize(CompactBuffer.scala:146)
        at org.apache.spark.util.collection.CompactBuffer.$plus$plus$eq(CompactBuffer.scala:92)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.mergeIfKeyExists(ExternalAppendOnlyMap.scala:355)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:402)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:302)
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at scala.collection.Iterator$$anon$12.next(Iterator.scala:445)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:131)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)
        ... 10 more

Driver stacktrace:
20/02/28 16:52:16 ERROR io.SparkHadoopWriter: Task attempt_20200228165146_0008_m_000004_0 aborted.
20/02/28 16:52:16 INFO executor.Executor: Executor interrupted and killed task 3.0 in stage 1.0 (TID 48), reason: Stage cancelled
20/02/28 16:52:16 INFO executor.Executor: Executor interrupted and killed task 4.0 in stage 1.0 (TID 49), reason: Stage cancelled
20/02/28 16:52:16 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 1.0 (TID 48, localhost, executor driver): TaskKilled (Stage cancelled)
20/02/28 16:52:16 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/02/28 16:52:16 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 1.0 (TID 49, localhost, executor driver): TaskKilled (Stage cancelled)
20/02/28 16:52:16 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/02/28 16:52:16 INFO scheduler.DAGScheduler: Job 0 failed: runJob at SparkHadoopWriter.scala:78, took 30.136851 s
20/02/28 16:52:16 ERROR io.SparkHadoopWriter: Aborting job job_20200228165146_0008.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 47, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:123)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:117)
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:115)
        at org.apache.spark.util.collection.CompactBuffer.growToSize(CompactBuffer.scala:146)
        at org.apache.spark.util.collection.CompactBuffer.$plus$plus$eq(CompactBuffer.scala:92)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.mergeIfKeyExists(ExternalAppendOnlyMap.scala:355)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:402)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:302)
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at scala.collection.Iterator$$anon$12.next(Iterator.scala:445)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:131)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)
        ... 10 more

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
        at scala.Option.foreach(Option.scala:257)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
        at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)
        at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1544)
        at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)
        at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
        at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1523)
        at wc.FollowerCount_RDDG$.main(FollowerCount_RDDG.scala:32)
        at wc.FollowerCount_RDDG.main(FollowerCount_RDDG.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.spark.SparkException: Task failed while writing rows
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:123)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:117)
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:115)
        at org.apache.spark.util.collection.CompactBuffer.growToSize(CompactBuffer.scala:146)
        at org.apache.spark.util.collection.CompactBuffer.$plus$plus$eq(CompactBuffer.scala:92)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.mergeIfKeyExists(ExternalAppendOnlyMap.scala:355)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:402)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:302)
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at scala.collection.Iterator$$anon$12.next(Iterator.scala:445)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:131)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)
        ... 10 more
Exception in thread "main" org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)
        at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1544)
        at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)
        at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1523)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
        at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1523)
        at wc.FollowerCount_RDDG$.main(FollowerCount_RDDG.scala:32)
        at wc.FollowerCount_RDDG.main(FollowerCount_RDDG.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 47, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:123)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:117)
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:115)
        at org.apache.spark.util.collection.CompactBuffer.growToSize(CompactBuffer.scala:146)
        at org.apache.spark.util.collection.CompactBuffer.$plus$plus$eq(CompactBuffer.scala:92)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.mergeIfKeyExists(ExternalAppendOnlyMap.scala:355)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:402)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:302)
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at scala.collection.Iterator$$anon$12.next(Iterator.scala:445)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:131)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)
        ... 10 more

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
        at scala.Option.foreach(Option.scala:257)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
        at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)
        ... 42 more
Caused by: org.apache.spark.SparkException: Task failed while writing rows
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:123)
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:117)
        at scala.reflect.ManifestFactory$$anon$9.newArray(Manifest.scala:115)
        at org.apache.spark.util.collection.CompactBuffer.growToSize(CompactBuffer.scala:146)
        at org.apache.spark.util.collection.CompactBuffer.$plus$plus$eq(CompactBuffer.scala:92)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$groupByKey$1$$anonfun$10.apply(PairRDDFunctions.scala:504)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.mergeIfKeyExists(ExternalAppendOnlyMap.scala:355)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:402)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:302)
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at scala.collection.Iterator$$anon$12.next(Iterator.scala:445)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:131)
        at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
        at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141)
        ... 10 more
20/02/28 16:52:16 INFO spark.SparkContext: Invoking stop() from shutdown hook
20/02/28 16:52:16 INFO server.AbstractConnector: Stopped Spark@240139e1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20/02/28 16:52:16 INFO ui.SparkUI: Stopped Spark web UI at http://10.110.81.16:4040
20/02/28 16:52:16 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/02/28 16:52:16 INFO memory.MemoryStore: MemoryStore cleared
20/02/28 16:52:16 INFO storage.BlockManager: BlockManager stopped
20/02/28 16:52:16 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
20/02/28 16:52:16 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/02/28 16:52:16 INFO spark.SparkContext: Successfully stopped SparkContext
20/02/28 16:52:16 INFO util.ShutdownHookManager: Shutdown hook called
20/02/28 16:52:16 INFO util.ShutdownHookManager: Deleting directory /private/var/folders/l1/_8wm1c852jqf0btwzdcc0m4r0000gn/T/spark-d4603fe6-8b5c-45b3-a544-dabe7e7f8485
20/02/28 16:52:16 INFO util.ShutdownHookManager: Deleting directory /private/var/folders/l1/_8wm1c852jqf0btwzdcc0m4r0000gn/T/spark-63d99c40-21bd-4401-b7a9-b222c0bfd6e3
make: *** [local] Error 1
(base) Srikanths-MacBook-Pro:Spark-trainglecount srikanthmandru$ 